// Enhanced native blur core with MediaPipe foundation and OpenCV blur engine
// Phase 1: MediaPipe segmentation integration preparation âœ…
// Phase 2: OpenCV native blur operations with GPU acceleration âœ…
// Phase 3: Advanced mask processing with morphological operations âœ…
// Phase 4: Smart compositing engine with intelligent image blending âœ…
// Phase 5: Performance optimization with memory management and multi-threading ðŸš§ IMPLEMENTING

#include <jni.h>
#include <android/log.h>
#include <vector>
#include <memory>
#include <string>
#include <chrono>
#include <thread>
#include <mutex>
#include <future>
#include <atomic>
#include <queue>
#include <functional>
#include <sstream>
#include <iomanip>
#include <condition_variable>

#define LOG_TAG "BlurCore"
#define LOGI(...) __android_log_print(ANDROID_LOG_INFO, LOG_TAG, __VA_ARGS__)
#define LOGE(...) __android_log_print(ANDROID_LOG_ERROR, LOG_TAG, __VA_ARGS__)

// Phase 1: MediaPipe integration flags
#ifdef ENABLE_MEDIAPIPE
#include "mediapipe/framework/api2/builder.h"
#include "mediapipe/framework/formats/image_frame.h"
#include "mediapipe/tasks/cc/vision/image_segmenter/image_segmenter.h"
#endif

// Phase 2: OpenCV integration for blur operations
#ifdef ENABLE_OPENCV
#include <opencv2/opencv.hpp>
#include <opencv2/imgproc.hpp>
#ifdef ENABLE_OPENCV_GPU
#include <opencv2/gpu/gpu.hpp>
#endif
#endif

namespace blurcore {

// Phase 2: OpenCV blur engine for high-performance image processing
class OpenCVBlurEngine {
private:
    bool initialized_ = false;
    bool gpu_available_ = false;
    
public:
    bool Initialize() {
        LOGI("OpenCVBlurEngine: Initializing");
        
#ifdef ENABLE_OPENCV
        try {
            // Check OpenCV version and GPU capabilities
            LOGI("OpenCVBlurEngine: OpenCV version %s", cv::getVersionString().c_str());
            
#ifdef ENABLE_OPENCV_GPU
            // Check if GPU acceleration is available
            if (cv::gpu::getCudaEnabledDeviceCount() > 0) {
                gpu_available_ = true;
                LOGI("OpenCVBlurEngine: GPU acceleration available (%d devices)", 
                     cv::gpu::getCudaEnabledDeviceCount());
            } else {
                LOGI("OpenCVBlurEngine: No GPU devices available, using CPU");
            }
#else
            LOGI("OpenCVBlurEngine: GPU support not compiled, using CPU");
#endif
            
            initialized_ = true;
            return true;
            
        } catch (const std::exception& e) {
            LOGE("OpenCVBlurEngine: Initialization failed: %s", e.what());
            return false;
        }
#else
        LOGI("OpenCVBlurEngine: OpenCV not enabled, using fallback");
        return false;
#endif
    }
    
    // Phase 2: High-performance Gaussian blur with multiple algorithms
    std::vector<uint8_t> ApplyGaussianBlur(const std::vector<uint8_t>& image_data, 
                                          int width, int height, int channels,
                                          double sigma, int blur_type) {
        if (!initialized_) {
            LOGI("OpenCVBlurEngine: Not initialized, returning original");
            return image_data;
        }
        
#ifdef ENABLE_OPENCV
        try {
            auto start_time = std::chrono::high_resolution_clock::now();
            
            // Convert input data to OpenCV Mat
            cv::Mat input_mat;
            if (channels == 3) {
                input_mat = cv::Mat(height, width, CV_8UC3, (void*)image_data.data()).clone();
            } else if (channels == 4) {
                input_mat = cv::Mat(height, width, CV_8UC4, (void*)image_data.data()).clone();
            } else {
                input_mat = cv::Mat(height, width, CV_8UC1, (void*)image_data.data()).clone();
            }
            
            cv::Mat output_mat;
            
            // Calculate kernel size from sigma
            int kernel_size = static_cast<int>(2 * std::ceil(3 * sigma) + 1);
            if (kernel_size % 2 == 0) kernel_size++; // Ensure odd kernel size
            
            // Apply blur based on type and capabilities
            switch (blur_type) {
                case 0: // Fast Gaussian (separable)
                    if (gpu_available_) {
                        ApplyGPUBlur(input_mat, output_mat, kernel_size, sigma);
                    } else {
                        cv::GaussianBlur(input_mat, output_mat, cv::Size(kernel_size, kernel_size), sigma);
                    }
                    break;
                    
                case 1: // Box blur (fastest)
                    cv::boxFilter(input_mat, output_mat, -1, cv::Size(kernel_size, kernel_size));
                    break;
                    
                case 2: // Motion blur
                    ApplyMotionBlur(input_mat, output_mat, kernel_size);
                    break;
                    
                default: // Standard Gaussian
                    cv::GaussianBlur(input_mat, output_mat, cv::Size(kernel_size, kernel_size), sigma);
                    break;
            }
            
            // Convert result back to byte vector
            std::vector<uint8_t> result(output_mat.total() * output_mat.elemSize());
            std::memcpy(result.data(), output_mat.data, result.size());
            
            auto end_time = std::chrono::high_resolution_clock::now();
            auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end_time - start_time);
            
            LOGI("OpenCVBlurEngine: Blur completed in %lld ms (GPU: %s, Type: %d)", 
                 duration.count(), gpu_available_ ? "yes" : "no", blur_type);
            
            return result;
            
        } catch (const std::exception& e) {
            LOGE("OpenCVBlurEngine: Blur operation failed: %s", e.what());
            return image_data; // Return original on error
        }
#else
        LOGI("OpenCVBlurEngine: OpenCV disabled, returning original");
        return image_data;
#endif
    }
    
    // Phase 2: Selective blur using mask
    std::vector<uint8_t> ApplySelectiveBlur(const std::vector<uint8_t>& image_data,
                                           const std::vector<uint8_t>& mask_data,
                                           int width, int height, int channels,
                                           double foreground_sigma, double background_sigma) {
        if (!initialized_) {
            return image_data;
        }
        
#ifdef ENABLE_OPENCV
        try {
            auto start_time = std::chrono::high_resolution_clock::now();
            
            // Convert image to OpenCV Mat
            cv::Mat image_mat;
            if (channels == 3) {
                image_mat = cv::Mat(height, width, CV_8UC3, (void*)image_data.data()).clone();
            } else if (channels == 4) {
                image_mat = cv::Mat(height, width, CV_8UC4, (void*)image_data.data()).clone();
            } else {
                image_mat = cv::Mat(height, width, CV_8UC1, (void*)image_data.data()).clone();
            }
            
            // Convert mask to OpenCV Mat
            cv::Mat mask_mat = cv::Mat(height, width, CV_8UC1, (void*)mask_data.data()).clone();
            
            // Create blurred versions
            cv::Mat fg_blurred, bg_blurred;
            
            // Calculate kernel sizes
            int fg_kernel = static_cast<int>(2 * std::ceil(3 * foreground_sigma) + 1);
            int bg_kernel = static_cast<int>(2 * std::ceil(3 * background_sigma) + 1);
            if (fg_kernel % 2 == 0) fg_kernel++;
            if (bg_kernel % 2 == 0) bg_kernel++;
            
            // Apply different blur amounts
            if (foreground_sigma > 0.1) {
                cv::GaussianBlur(image_mat, fg_blurred, cv::Size(fg_kernel, fg_kernel), foreground_sigma);
            } else {
                fg_blurred = image_mat.clone();
            }
            
            if (background_sigma > 0.1) {
                cv::GaussianBlur(image_mat, bg_blurred, cv::Size(bg_kernel, bg_kernel), background_sigma);
            } else {
                bg_blurred = image_mat.clone();
            }
            
            // Blend based on mask
            cv::Mat result_mat;
            cv::Mat mask_normalized;
            mask_mat.convertTo(mask_normalized, CV_32F, 1.0/255.0);
            
            // Convert images to float for blending
            cv::Mat fg_float, bg_float;
            fg_blurred.convertTo(fg_float, CV_32F);
            bg_blurred.convertTo(bg_float, CV_32F);
            
            // Blend: result = foreground * mask + background * (1 - mask)
            cv::Mat result_float;
            std::vector<cv::Mat> fg_channels, bg_channels, result_channels;
            
            if (channels > 1) {
                cv::split(fg_float, fg_channels);
                cv::split(bg_float, bg_channels);
                
                for (int i = 0; i < channels; i++) {
                    cv::Mat channel_result;
                    cv::multiply(fg_channels[i], mask_normalized, channel_result);
                    cv::Mat bg_contribution;
                    cv::multiply(bg_channels[i], cv::Scalar::all(1.0) - mask_normalized, bg_contribution);
                    cv::add(channel_result, bg_contribution, channel_result);
                    result_channels.push_back(channel_result);
                }
                
                cv::merge(result_channels, result_float);
            } else {
                cv::multiply(fg_float, mask_normalized, result_float);
                cv::Mat bg_contribution;
                cv::multiply(bg_float, cv::Scalar::all(1.0) - mask_normalized, bg_contribution);
                cv::add(result_float, bg_contribution, result_float);
            }
            
            // Convert back to uint8
            result_float.convertTo(result_mat, CV_8U);
            
            // Convert result back to byte vector
            std::vector<uint8_t> result(result_mat.total() * result_mat.elemSize());
            std::memcpy(result.data(), result_mat.data, result.size());
            
            auto end_time = std::chrono::high_resolution_clock::now();
            auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end_time - start_time);
            
            LOGI("OpenCVBlurEngine: Selective blur completed in %lld ms", duration.count());
            
            return result;
            
        } catch (const std::exception& e) {
            LOGE("OpenCVBlurEngine: Selective blur failed: %s", e.what());
            return image_data;
        }
#else
        return image_data;
#endif
    }
    
    bool IsInitialized() const { return initialized_; }
    bool IsGPUAvailable() const { return gpu_available_; }
    
    void Cleanup() {
        if (initialized_) {
            LOGI("OpenCVBlurEngine: Cleaning up");
            initialized_ = false;
            gpu_available_ = false;
        }
    }

private:
#ifdef ENABLE_OPENCV
#ifdef ENABLE_OPENCV_GPU
    void ApplyGPUBlur(const cv::Mat& input, cv::Mat& output, int kernel_size, double sigma) {
        try {
            cv::gpu::GpuMat gpu_input, gpu_output;
            gpu_input.upload(input);
            
            cv::gpu::GaussianBlur(gpu_input, gpu_output, cv::Size(kernel_size, kernel_size), sigma);
            
            gpu_output.download(output);
            LOGI("OpenCVBlurEngine: GPU blur applied successfully");
        } catch (const std::exception& e) {
            LOGE("OpenCVBlurEngine: GPU blur failed, falling back to CPU: %s", e.what());
            cv::GaussianBlur(input, output, cv::Size(kernel_size, kernel_size), sigma);
        }
    }
#else
    void ApplyGPUBlur(const cv::Mat& input, cv::Mat& output, int kernel_size, double sigma) {
        // Fallback to CPU if GPU not available
        cv::GaussianBlur(input, output, cv::Size(kernel_size, kernel_size), sigma);
    }
#endif

    void ApplyMotionBlur(const cv::Mat& input, cv::Mat& output, int kernel_size) {
        // Create motion blur kernel (horizontal)
        cv::Mat motion_kernel = cv::getRotationMatrix2D(cv::Point2f(kernel_size/2, kernel_size/2), 0, 1.0);
        motion_kernel = cv::Mat::zeros(kernel_size, kernel_size, CV_32F);
        motion_kernel.row(kernel_size/2).setTo(cv::Scalar(1.0/kernel_size));
        
        cv::filter2D(input, output, -1, motion_kernel);
    }
#endif
};

// Phase 3: Advanced mask processing with morphological operations
class AdvancedMaskProcessor {
private:
    bool initialized_ = false;
    
public:
    bool Initialize() {
        LOGI("AdvancedMaskProcessor: Initializing");
        
#ifdef ENABLE_OPENCV
        initialized_ = true;
        return true;
#else
        LOGI("AdvancedMaskProcessor: OpenCV not available, using fallback");
        return false;
#endif
    }
    
    // Phase 3: Morphological operations for mask refinement
    std::vector<uint8_t> RefineMask(const std::vector<uint8_t>& mask_data,
                                   int width, int height,
                                   int operation_type, int kernel_size,
                                   int iterations = 1) {
        if (!initialized_) {
            return mask_data;
        }
        
#ifdef ENABLE_OPENCV
        try {
            auto start_time = std::chrono::high_resolution_clock::now();
            
            // Convert mask to OpenCV Mat
            cv::Mat mask_mat = cv::Mat(height, width, CV_8UC1, (void*)mask_data.data()).clone();
            
            // Create morphological kernel
            cv::Mat kernel;
            int morph_type = cv::MORPH_ELLIPSE; // Default to elliptical kernel
            kernel = cv::getStructuringElement(morph_type, cv::Size(kernel_size, kernel_size));
            
            cv::Mat result_mat;
            
            // Apply morphological operation
            switch (operation_type) {
                case 0: // Dilate - expand mask areas
                    cv::dilate(mask_mat, result_mat, kernel, cv::Point(-1, -1), iterations);
                    break;
                    
                case 1: // Erode - shrink mask areas
                    cv::erode(mask_mat, result_mat, kernel, cv::Point(-1, -1), iterations);
                    break;
                    
                case 2: // Opening - erode then dilate (remove noise)
                    cv::morphologyEx(mask_mat, result_mat, cv::MORPH_OPEN, kernel, cv::Point(-1, -1), iterations);
                    break;
                    
                case 3: // Closing - dilate then erode (fill gaps)
                    cv::morphologyEx(mask_mat, result_mat, cv::MORPH_CLOSE, kernel, cv::Point(-1, -1), iterations);
                    break;
                    
                case 4: // Gradient - difference between dilation and erosion (edges)
                    cv::morphologyEx(mask_mat, result_mat, cv::MORPH_GRADIENT, kernel, cv::Point(-1, -1), iterations);
                    break;
                    
                default:
                    result_mat = mask_mat.clone();
                    break;
            }
            
            // Convert result back to byte vector
            std::vector<uint8_t> result(result_mat.total());
            std::memcpy(result.data(), result_mat.data, result.size());
            
            auto end_time = std::chrono::high_resolution_clock::now();
            auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end_time - start_time);
            
            LOGI("AdvancedMaskProcessor: Morphological operation %d completed in %lld ms", 
                 operation_type, duration.count());
            
            return result;
            
        } catch (const std::exception& e) {
            LOGE("AdvancedMaskProcessor: Morphological operation failed: %s", e.what());
            return mask_data;
        }
#else
        return mask_data;
#endif
    }
    
    // Phase 3: Edge smoothing for natural mask transitions
    std::vector<uint8_t> SmoothMaskEdges(const std::vector<uint8_t>& mask_data,
                                        int width, int height,
                                        double sigma = 2.0,
                                        int feather_radius = 5) {
        if (!initialized_) {
            return mask_data;
        }
        
#ifdef ENABLE_OPENCV
        try {
            auto start_time = std::chrono::high_resolution_clock::now();
            
            // Convert mask to OpenCV Mat
            cv::Mat mask_mat = cv::Mat(height, width, CV_8UC1, (void*)mask_data.data()).clone();
            cv::Mat result_mat;
            
            // Apply Gaussian blur for soft edges
            int kernel_size = 2 * feather_radius + 1;
            cv::GaussianBlur(mask_mat, result_mat, cv::Size(kernel_size, kernel_size), sigma);
            
            // Optional: Apply distance transform for more natural falloff
            if (feather_radius > 3) {
                cv::Mat dist_transform, normalized_dist;
                cv::distanceTransform(mask_mat, dist_transform, cv::DIST_L2, 3);
                
                // Normalize and apply falloff
                double max_dist;
                cv::minMaxLoc(dist_transform, nullptr, &max_dist);
                
                if (max_dist > 0) {
                    dist_transform.convertTo(normalized_dist, CV_8U, 255.0 / max_dist);
                    
                    // Blend with Gaussian blur result
                    cv::addWeighted(result_mat, 0.7, normalized_dist, 0.3, 0, result_mat);
                }
            }
            
            // Convert result back to byte vector
            std::vector<uint8_t> result(result_mat.total());
            std::memcpy(result.data(), result_mat.data, result.size());
            
            auto end_time = std::chrono::high_resolution_clock::now();
            auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end_time - start_time);
            
            LOGI("AdvancedMaskProcessor: Edge smoothing completed in %lld ms", duration.count());
            
            return result;
            
        } catch (const std::exception& e) {
            LOGE("AdvancedMaskProcessor: Edge smoothing failed: %s", e.what());
            return mask_data;
        }
#else
        return mask_data;
#endif
    }
    
    // Phase 3: Intelligent mask cleanup and optimization
    std::vector<uint8_t> OptimizeMask(const std::vector<uint8_t>& mask_data,
                                     int width, int height,
                                     double noise_threshold = 0.1,
                                     int min_component_size = 100) {
        if (!initialized_) {
            return mask_data;
        }
        
#ifdef ENABLE_OPENCV
        try {
            auto start_time = std::chrono::high_resolution_clock::now();
            
            // Convert mask to OpenCV Mat
            cv::Mat mask_mat = cv::Mat(height, width, CV_8UC1, (void*)mask_data.data()).clone();
            cv::Mat result_mat = mask_mat.clone();
            
            // Step 1: Remove small noise components
            cv::Mat labels, stats, centroids;
            int num_labels = cv::connectedComponentsWithStats(mask_mat, labels, stats, centroids);
            
            // Filter out small components
            for (int i = 1; i < num_labels; i++) {
                int area = stats.at<int>(i, cv::CC_STAT_AREA);
                if (area < min_component_size) {
                    // Remove small component
                    cv::Mat component_mask = (labels == i);
                    result_mat.setTo(0, component_mask);
                }
            }
            
            // Step 2: Apply bilateral filter for edge-preserving smoothing
            cv::Mat bilateral_result;
            cv::bilateralFilter(result_mat, bilateral_result, 9, 75, 75);
            
            // Step 3: Threshold to maintain binary mask
            cv::threshold(bilateral_result, result_mat, 127, 255, cv::THRESH_BINARY);
            
            // Step 4: Final morphological closing to smooth boundaries
            cv::Mat close_kernel = cv::getStructuringElement(cv::MORPH_ELLIPSE, cv::Size(5, 5));
            cv::morphologyEx(result_mat, result_mat, cv::MORPH_CLOSE, close_kernel);
            
            // Convert result back to byte vector
            std::vector<uint8_t> result(result_mat.total());
            std::memcpy(result.data(), result_mat.data, result.size());
            
            auto end_time = std::chrono::high_resolution_clock::now();
            auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end_time - start_time);
            
            LOGI("AdvancedMaskProcessor: Mask optimization completed in %lld ms", duration.count());
            
            return result;
            
        } catch (const std::exception& e) {
            LOGE("AdvancedMaskProcessor: Mask optimization failed: %s", e.what());
            return mask_data;
        }
#else
        return mask_data;
#endif
    }
    
    // Phase 3: Advanced mask blending and feathering
    std::vector<uint8_t> CreateFeatheredMask(const std::vector<uint8_t>& mask_data,
                                            int width, int height,
                                            int inner_feather = 10,
                                            int outer_feather = 15) {
        if (!initialized_) {
            return mask_data;
        }
        
#ifdef ENABLE_OPENCV
        try {
            auto start_time = std::chrono::high_resolution_clock::now();
            
            // Convert mask to OpenCV Mat
            cv::Mat mask_mat = cv::Mat(height, width, CV_8UC1, (void*)mask_data.data()).clone();
            
            // Create distance transforms for inner and outer feathering
            cv::Mat dist_inner, dist_outer;
            cv::Mat inverted_mask = 255 - mask_mat;
            
            // Inner feathering (from edge inward)
            cv::distanceTransform(mask_mat, dist_inner, cv::DIST_L2, 3);
            
            // Outer feathering (from edge outward)
            cv::distanceTransform(inverted_mask, dist_outer, cv::DIST_L2, 3);
            
            // Create feathered mask
            cv::Mat result_mat = cv::Mat::zeros(height, width, CV_8UC1);
            
            for (int y = 0; y < height; y++) {
                for (int x = 0; x < width; x++) {
                    uint8_t original_value = mask_mat.at<uint8_t>(y, x);
                    float inner_dist = dist_inner.at<float>(y, x);
                    float outer_dist = dist_outer.at<float>(y, x);
                    
                    float alpha = 1.0f;
                    
                    if (original_value > 127) {
                        // Inside mask - apply inner feathering
                        if (inner_dist < inner_feather) {
                            alpha = inner_dist / inner_feather;
                        }
                    } else {
                        // Outside mask - apply outer feathering
                        if (outer_dist < outer_feather) {
                            alpha = 1.0f - (outer_dist / outer_feather);
                        } else {
                            alpha = 0.0f;
                        }
                    }
                    
                    result_mat.at<uint8_t>(y, x) = static_cast<uint8_t>(alpha * 255);
                }
            }
            
            // Convert result back to byte vector
            std::vector<uint8_t> result(result_mat.total());
            std::memcpy(result.data(), result_mat.data, result.size());
            
            auto end_time = std::chrono::high_resolution_clock::now();
            auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end_time - start_time);
            
            LOGI("AdvancedMaskProcessor: Feathered mask creation completed in %lld ms", duration.count());
            
            return result;
            
        } catch (const std::exception& e) {
            LOGE("AdvancedMaskProcessor: Feathered mask creation failed: %s", e.what());
            return mask_data;
        }
#else
        return mask_data;
#endif
    }
    
    bool IsInitialized() const { return initialized_; }
    
    void Cleanup() {
        if (initialized_) {
            LOGI("AdvancedMaskProcessor: Cleaning up");
            initialized_ = false;
        }
    }
};

// Phase 1: MediaPipe segmenter wrapper
class MediaPipeSegmenter {
private:
    bool initialized_ = false;
    
#ifdef ENABLE_MEDIAPIPE
    std::unique_ptr<mediapipe::tasks::vision::ImageSegmenter> segmenter_;
#endif

public:
    bool Initialize(const std::string& model_path) {
        LOGI("MediaPipeSegmenter: Initializing with model: %s", model_path.c_str());
        
#ifdef ENABLE_MEDIAPIPE
        // TODO Phase 1: MediaPipe initialization
        auto options = std::make_unique<mediapipe::tasks::vision::ImageSegmenterOptions>();
        options->base_options.model_asset_path = model_path;
        
        auto result = mediapipe::tasks::vision::ImageSegmenter::Create(std::move(options));
        if (result.ok()) {
            segmenter_ = std::move(result.value());
            initialized_ = true;
            LOGI("MediaPipeSegmenter: Successfully initialized");
            return true;
        } else {
            LOGE("MediaPipeSegmenter: Failed to initialize: %s", result.status().message().data());
            return false;
        }
#else
        LOGI("MediaPipeSegmenter: MediaPipe not enabled, using fallback");
        initialized_ = false;
        return false;
#endif
    }
    
    std::vector<uint8_t> Segment(const std::vector<uint8_t>& image_data, int width, int height) {
        if (!initialized_) {
            LOGI("MediaPipeSegmenter: Not initialized, returning empty mask");
            return std::vector<uint8_t>();
        }
        
#ifdef ENABLE_MEDIAPIPE
        // TODO Phase 1: Actual segmentation implementation
        LOGI("MediaPipeSegmenter: Processing %dx%d image", width, height);
        // Placeholder: return empty for now
        return std::vector<uint8_t>();
#else
        LOGI("MediaPipeSegmenter: MediaPipe disabled, using fallback");
        return std::vector<uint8_t>();
#endif
    }
    
    bool IsInitialized() const { return initialized_; }
    
    void Cleanup() {
        if (initialized_) {
            LOGI("MediaPipeSegmenter: Cleaning up");
#ifdef ENABLE_MEDIAPIPE
            segmenter_.reset();
#endif
            initialized_ = false;
        }
    }
};

// ================================================================================
// Phase 4: Smart Compositing Engine
// ================================================================================

class SmartCompositingEngine {
private:
    bool initialized_ = false;
    
    // Color space conversion matrices (OpenCV-dependent)
    struct ColorSpaceConverter {
        // RGB to HSV conversion for better hue/saturation preservation
        static void RGBtoHSV(const std::vector<uint8_t>& rgb, std::vector<uint8_t>& hsv, int width, int height) {
#ifdef ENABLE_OPENCV
            cv::Mat rgb_mat(height, width, CV_8UC3, (void*)rgb.data());
            cv::Mat hsv_mat;
            cv::cvtColor(rgb_mat, hsv_mat, cv::COLOR_RGB2HSV);
            hsv.resize(hsv_mat.total() * hsv_mat.elemSize());
            std::memcpy(hsv.data(), hsv_mat.data, hsv.size());
#else
            // Fallback: copy input to output
            hsv = rgb;
#endif
        }
        
        // RGB to LAB for perceptual uniformity
        static void RGBtoLAB(const std::vector<uint8_t>& rgb, std::vector<uint8_t>& lab, int width, int height) {
#ifdef ENABLE_OPENCV
            cv::Mat rgb_mat(height, width, CV_8UC3, (void*)rgb.data());
            cv::Mat lab_mat;
            cv::cvtColor(rgb_mat, lab_mat, cv::COLOR_RGB2Lab);
            lab.resize(lab_mat.total() * lab_mat.elemSize());
            std::memcpy(lab.data(), lab_mat.data, lab.size());
#else
            // Fallback: copy input to output
            lab = rgb;
#endif
        }
        
        // Convert back to RGB
        static void HSVtoRGB(const std::vector<uint8_t>& hsv, std::vector<uint8_t>& rgb, int width, int height) {
#ifdef ENABLE_OPENCV
            cv::Mat hsv_mat(height, width, CV_8UC3, (void*)hsv.data());
            cv::Mat rgb_mat;
            cv::cvtColor(hsv_mat, rgb_mat, cv::COLOR_HSV2RGB);
            rgb.resize(rgb_mat.total() * rgb_mat.elemSize());
            std::memcpy(rgb.data(), rgb_mat.data, rgb.size());
#else
            // Fallback: copy input to output
            rgb = hsv;
#endif
        }
        
        static void LABtoRGB(const std::vector<uint8_t>& lab, std::vector<uint8_t>& rgb, int width, int height) {
#ifdef ENABLE_OPENCV
            cv::Mat lab_mat(height, width, CV_8UC3, (void*)lab.data());
            cv::Mat rgb_mat;
            cv::cvtColor(lab_mat, rgb_mat, cv::COLOR_Lab2RGB);
            rgb.resize(rgb_mat.total() * rgb_mat.elemSize());
            std::memcpy(rgb.data(), rgb_mat.data, rgb.size());
#else
            // Fallback: copy input to output
            rgb = lab;
#endif
        }
    };

public:
    SmartCompositingEngine() = default;
    ~SmartCompositingEngine() = default;

    bool Initialize() {
        if (initialized_) return true;
        
        auto start = std::chrono::high_resolution_clock::now();
        LOGI("SmartCompositingEngine: Initializing intelligent compositing...");

#ifdef ENABLE_OPENCV
        if (!cv::getBuildInformation().empty()) {
            initialized_ = true;
            auto end = std::chrono::high_resolution_clock::now();
            auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end - start);
            LOGI("SmartCompositingEngine: Initialized successfully (%ldms)", duration.count());
            return true;
        }
#endif
        
        LOGE("SmartCompositingEngine: OpenCV not available");
        return false;
    }

    bool IsInitialized() const { return initialized_; }

    // Multi-layer alpha blending with smart edge preservation
    std::vector<uint8_t> BlendLayers(
        const std::vector<uint8_t>& base_image,
        const std::vector<uint8_t>& overlay_image, 
        const std::vector<uint8_t>& mask,
        int width, int height,
        double blend_strength = 1.0) {
        
        if (!initialized_) {
            LOGE("SmartCompositingEngine: Not initialized");
            return {};
        }

#ifdef ENABLE_OPENCV
        auto start = std::chrono::high_resolution_clock::now();
        
        try {
            // Convert input data to OpenCV matrices
            cv::Mat base(height, width, CV_8UC3, (void*)base_image.data());
            cv::Mat overlay(height, width, CV_8UC3, (void*)overlay_image.data());
            cv::Mat alpha_mask(height, width, CV_8UC1, (void*)mask.data());
            
            // Normalize mask to [0,1] range for blending
            cv::Mat normalized_mask;
            alpha_mask.convertTo(normalized_mask, CV_32F, 1.0/255.0);
            
            // Apply blend strength
            normalized_mask *= blend_strength;
            
            // Smart edge preservation - detect edges in base image
            cv::Mat base_gray, edges;
            cv::cvtColor(base, base_gray, cv::COLOR_BGR2GRAY);
            cv::Canny(base_gray, edges, 50, 150);
            
            // Reduce blending near edges to preserve detail
            cv::Mat edge_mask;
            edges.convertTo(edge_mask, CV_32F, 1.0/255.0);
            cv::GaussianBlur(edge_mask, edge_mask, cv::Size(5, 5), 1.0);
            normalized_mask = normalized_mask.mul(1.0 - edge_mask * 0.3);
            
            // Convert to floating point for precise blending
            cv::Mat base_f, overlay_f;
            base.convertTo(base_f, CV_32FC3, 1.0/255.0);
            overlay.convertTo(overlay_f, CV_32FC3, 1.0/255.0);
            
            // Multi-channel alpha blending
            cv::Mat result_f = cv::Mat::zeros(base.size(), CV_32FC3);
            std::vector<cv::Mat> base_channels, overlay_channels, result_channels(3);
            cv::split(base_f, base_channels);
            cv::split(overlay_f, overlay_channels);
            
            for (int c = 0; c < 3; c++) {
                result_channels[c] = base_channels[c].mul(1.0 - normalized_mask) + 
                                   overlay_channels[c].mul(normalized_mask);
            }
            
            cv::merge(result_channels, result_f);
            
            // Convert back to 8-bit
            cv::Mat result;
            result_f.convertTo(result, CV_8UC3, 255.0);
            
            // Copy result to output vector
            std::vector<uint8_t> output(result.total() * result.elemSize());
            std::memcpy(output.data(), result.data, output.size());
            
            auto end = std::chrono::high_resolution_clock::now();
            auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end - start);
            LOGI("SmartCompositingEngine: Blend completed (%ldms)", duration.count());
            
            return output;
            
        } catch (const std::exception& e) {
            LOGE("SmartCompositingEngine: Blending failed: %s", e.what());
            return {};
        }
#else
        LOGE("SmartCompositingEngine: OpenCV not available");
        return {};
#endif
    }

    // Advanced color space blending for natural transitions
    std::vector<uint8_t> AdvancedColorBlend(
        const std::vector<uint8_t>& base_image,
        const std::vector<uint8_t>& overlay_image,
        const std::vector<uint8_t>& mask,
        int width, int height,
        const std::string& color_space = "HSV") {
        
        if (!initialized_) {
            LOGE("SmartCompositingEngine: Not initialized");
            return {};
        }

#ifdef ENABLE_OPENCV
        auto start = std::chrono::high_resolution_clock::now();
        
        try {
            cv::Mat base(height, width, CV_8UC3, (void*)base_image.data());
            cv::Mat overlay(height, width, CV_8UC3, (void*)overlay_image.data());
            cv::Mat alpha_mask(height, width, CV_8UC1, (void*)mask.data());
            
            cv::Mat base_converted, overlay_converted, result_converted;
            
            if (color_space == "HSV") {
                ColorSpaceConverter::RGBtoHSV(base, base_converted);
                ColorSpaceConverter::RGBtoHSV(overlay, overlay_converted);
            } else if (color_space == "LAB") {
                ColorSpaceConverter::RGBtoLAB(base, base_converted);
                ColorSpaceConverter::RGBtoLAB(overlay, overlay_converted);
            } else {
                base_converted = base.clone();
                overlay_converted = overlay.clone();
            }
            
            // Normalize mask
            cv::Mat normalized_mask;
            alpha_mask.convertTo(normalized_mask, CV_32F, 1.0/255.0);
            
            // Blend in converted color space
            cv::Mat base_f, overlay_f;
            base_converted.convertTo(base_f, CV_32FC3, 1.0/255.0);
            overlay_converted.convertTo(overlay_f, CV_32FC3, 1.0/255.0);
            
            cv::Mat result_f = cv::Mat::zeros(base.size(), CV_32FC3);
            std::vector<cv::Mat> base_channels, overlay_channels, result_channels(3);
            cv::split(base_f, base_channels);
            cv::split(overlay_f, overlay_channels);
            
            for (int c = 0; c < 3; c++) {
                result_channels[c] = base_channels[c].mul(1.0 - normalized_mask) + 
                                   overlay_channels[c].mul(normalized_mask);
            }
            
            cv::merge(result_channels, result_f);
            
            // Convert back to 8-bit in converted space
            result_f.convertTo(result_converted, CV_8UC3, 255.0);
            
            // Convert back to RGB
            cv::Mat final_result;
            if (color_space == "HSV") {
                ColorSpaceConverter::HSVtoRGB(result_converted, final_result);
            } else if (color_space == "LAB") {
                ColorSpaceConverter::LABtoRGB(result_converted, final_result);
            } else {
                final_result = result_converted.clone();
            }
            
            std::vector<uint8_t> output(final_result.total() * final_result.elemSize());
            std::memcpy(output.data(), final_result.data, output.size());
            
            auto end = std::chrono::high_resolution_clock::now();
            auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end - start);
            LOGI("SmartCompositingEngine: Advanced color blend completed (%ldms)", duration.count());
            
            return output;
            
        } catch (const std::exception& e) {
            LOGE("SmartCompositingEngine: Advanced color blending failed: %s", e.what());
            return {};
        }
#else
        LOGE("SmartCompositingEngine: OpenCV not available");
        return {};
#endif
    }

    // Gradient domain compositing for seamless transitions
    std::vector<uint8_t> GradientDomainComposite(
        const std::vector<uint8_t>& base_image,
        const std::vector<uint8_t>& overlay_image,
        const std::vector<uint8_t>& mask,
        int width, int height) {
        
        if (!initialized_) {
            LOGE("SmartCompositingEngine: Not initialized");
            return {};
        }

#ifdef ENABLE_OPENCV
        auto start = std::chrono::high_resolution_clock::now();
        
        try {
            cv::Mat base(height, width, CV_8UC3, (void*)base_image.data());
            cv::Mat overlay(height, width, CV_8UC3, (void*)overlay_image.data());
            cv::Mat alpha_mask(height, width, CV_8UC1, (void*)mask.data());
            
            // Compute gradients for both images
            cv::Mat base_gray, overlay_gray;
            cv::cvtColor(base, base_gray, cv::COLOR_BGR2GRAY);
            cv::cvtColor(overlay, overlay_gray, cv::COLOR_BGR2GRAY);
            
            cv::Mat grad_x_base, grad_y_base, grad_x_overlay, grad_y_overlay;
            cv::Sobel(base_gray, grad_x_base, CV_32F, 1, 0, 3);
            cv::Sobel(base_gray, grad_y_base, CV_32F, 0, 1, 3);
            cv::Sobel(overlay_gray, grad_x_overlay, CV_32F, 1, 0, 3);
            cv::Sobel(overlay_gray, grad_y_overlay, CV_32F, 0, 1, 3);
            
            // Blend gradients based on mask
            cv::Mat mask_f;
            alpha_mask.convertTo(mask_f, CV_32F, 1.0/255.0);
            
            cv::Mat blended_grad_x = grad_x_base.mul(1.0 - mask_f) + grad_x_overlay.mul(mask_f);
            cv::Mat blended_grad_y = grad_y_base.mul(1.0 - mask_f) + grad_y_overlay.mul(mask_f);
            
            // Reconstruct image from blended gradients (simplified Poisson reconstruction)
            cv::Mat magnitude;
            cv::magnitude(blended_grad_x, blended_grad_y, magnitude);
            
            // Use bilateral filter to smooth while preserving edges
            cv::Mat base_f, result_f;
            base.convertTo(base_f, CV_32FC3, 1.0/255.0);
            cv::bilateralFilter(base_f, result_f, 9, 75, 75);
            
            // Blend based on gradient magnitude and mask
            cv::Mat gradient_weight;
            cv::normalize(magnitude, gradient_weight, 0, 1, cv::NORM_MINMAX);
            gradient_weight = gradient_weight.mul(mask_f);
            
            cv::Mat overlay_f;
            overlay.convertTo(overlay_f, CV_32FC3, 1.0/255.0);
            
            // Apply gradient-weighted blending
            std::vector<cv::Mat> base_channels, overlay_channels, result_channels(3);
            cv::split(base_f, base_channels);
            cv::split(overlay_f, overlay_channels);
            
            for (int c = 0; c < 3; c++) {
                result_channels.push_back(
                    base_channels[c].mul(1.0 - gradient_weight) + 
                    overlay_channels[c].mul(gradient_weight)
                );
            }
            
            cv::Mat final_result_f;
            cv::merge(result_channels, final_result_f);
            
            cv::Mat final_result;
            final_result_f.convertTo(final_result, CV_8UC3, 255.0);
            
            std::vector<uint8_t> output(final_result.total() * final_result.elemSize());
            std::memcpy(output.data(), final_result.data, output.size());
            
            auto end = std::chrono::high_resolution_clock::now();
            auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end - start);
            LOGI("SmartCompositingEngine: Gradient domain composite completed (%ldms)", duration.count());
            
            return output;
            
        } catch (const std::exception& e) {
            LOGE("SmartCompositingEngine: Gradient domain compositing failed: %s", e.what());
            return {};
        }
#else
        LOGE("SmartCompositingEngine: OpenCV not available");
        return {};
#endif
    }

    void Cleanup() {
        if (initialized_) {
            LOGI("SmartCompositingEngine: Cleaning up");
            initialized_ = false;
        }
    }
};

// ================================================================================
// Phase 5: Performance Optimization Engine
// ================================================================================

class PerformanceOptimizationEngine {
private:
    bool initialized_ = false;
    std::atomic<bool> should_stop_{false};
    
    // Thread pool for parallel processing
    class ThreadPool {
    private:
        std::vector<std::thread> workers;
        std::queue<std::function<void()>> tasks;
        std::mutex queue_mutex;
        std::condition_variable condition;
        bool stop = false;
        
    public:
        explicit ThreadPool(size_t num_threads = std::thread::hardware_concurrency()) {
            for (size_t i = 0; i < num_threads; ++i) {
                workers.emplace_back([this] {
                    for (;;) {
                        std::function<void()> task;
                        {
                            std::unique_lock<std::mutex> lock(this->queue_mutex);
                            this->condition.wait(lock, [this] { return this->stop || !this->tasks.empty(); });
                            if (this->stop && this->tasks.empty()) return;
                            task = std::move(this->tasks.front());
                            this->tasks.pop();
                        }
                        task();
                    }
                });
            }
        }
        
        template<class F, class... Args>
        auto enqueue(F&& f, Args&&... args) -> std::future<decltype(f(args...))> {
            using return_type = decltype(f(args...));
            
            auto task = std::make_shared<std::packaged_task<return_type()>>(
                std::bind(std::forward<F>(f), std::forward<Args>(args)...)
            );
            
            std::future<return_type> res = task->get_future();
            {
                std::unique_lock<std::mutex> lock(queue_mutex);
                if (stop) {
                    LOGE("ThreadPool: Cannot enqueue on stopped pool");
                    return std::future<return_type>();
                }
                tasks.emplace([task]() { (*task)(); });
            }
            condition.notify_one();
            return res;
        }
        
        ~ThreadPool() {
            {
                std::unique_lock<std::mutex> lock(queue_mutex);
                stop = true;
            }
            condition.notify_all();
            for (std::thread &worker : workers) worker.join();
        }
    };
    
    std::unique_ptr<ThreadPool> thread_pool_;
    
    // Memory pool for efficient allocation
    class MemoryPool {
    private:
        struct Block {
            std::vector<uint8_t> data;
            bool in_use = false;
            size_t size = 0;
        };
        
        std::vector<Block> pool_;
        mutable std::mutex pool_mutex_;
        static constexpr size_t MAX_POOL_SIZE = 32;
        static constexpr size_t BLOCK_SIZE = 1024 * 1024 * 4; // 4MB blocks
        
    public:
        std::vector<uint8_t>* acquireBuffer(size_t required_size) {
            std::lock_guard<std::mutex> lock(pool_mutex_);
            
            // Find an available block of sufficient size
            for (auto& block : pool_) {
                if (!block.in_use && block.data.size() >= required_size) {
                    block.in_use = true;
                    return &block.data;
                }
            }
            
            // Create new block if pool isn't full
            if (pool_.size() < MAX_POOL_SIZE) {
                pool_.emplace_back();
                auto& new_block = pool_.back();
                new_block.data.resize(std::max(required_size, BLOCK_SIZE));
                new_block.in_use = true;
                new_block.size = required_size;
                return &new_block.data;
            }
            
            // Pool is full, return nullptr to indicate fallback to direct allocation
            return nullptr;
        }
        
        void releaseBuffer(std::vector<uint8_t>* buffer) {
            std::lock_guard<std::mutex> lock(pool_mutex_);
            
            for (auto& block : pool_) {
                if (&block.data == buffer) {
                    block.in_use = false;
                    break;
                }
            }
        }
        
        void cleanup() {
            std::lock_guard<std::mutex> lock(pool_mutex_);
            pool_.clear();
        }
        
        size_t getPoolSize() const {
            std::lock_guard<std::mutex> lock(pool_mutex_);
            return pool_.size();
        }
    };
    
    std::unique_ptr<MemoryPool> memory_pool_;
    
    // Performance profiler
    struct PerformanceMetrics {
        std::atomic<uint64_t> total_operations{0};
        std::atomic<uint64_t> total_processing_time{0};
        std::atomic<uint64_t> memory_allocations{0};
        std::atomic<uint64_t> gpu_operations{0};
        std::mutex metrics_mutex;
        
        void recordOperation(uint64_t duration_ms) {
            total_operations.fetch_add(1);
            total_processing_time.fetch_add(duration_ms);
        }
        
        void recordMemoryAllocation() {
            memory_allocations.fetch_add(1);
        }
        
        void recordGPUOperation() {
            gpu_operations.fetch_add(1);
        }
        
        double getAverageProcessingTime() const {
            uint64_t ops = total_operations.load();
            if (ops == 0) return 0.0;
            return static_cast<double>(total_processing_time.load()) / ops;
        }
    };
    
    PerformanceMetrics metrics_;

public:
    PerformanceOptimizationEngine() = default;
    ~PerformanceOptimizationEngine() = default;

    bool Initialize() {
        if (initialized_) return true;
        
        auto start = std::chrono::high_resolution_clock::now();
        LOGI("PerformanceOptimizationEngine: Initializing optimization systems...");

        // Initialize thread pool
        thread_pool_ = std::make_unique<ThreadPool>();
        
        // Initialize memory pool
        memory_pool_ = std::make_unique<MemoryPool>();
        
        initialized_ = true;
        
        auto end = std::chrono::high_resolution_clock::now();
        auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end - start);
        LOGI("PerformanceOptimizationEngine: Initialized successfully (%lldms)", (long long)duration.count());
        LOGI("PerformanceOptimizationEngine: Thread pool with %d threads, memory pool ready", 
             std::thread::hardware_concurrency());
        
        return true;
    }

    bool IsInitialized() const { return initialized_; }

    // Parallel image processing with thread pool
    std::vector<uint8_t> ProcessImageParallel(
        const std::vector<uint8_t>& image_data,
        int width, int height,
        std::function<void(uint8_t*, int, int, int, int)> processor) {
        
        if (!initialized_) {
            LOGE("PerformanceOptimizationEngine: Not initialized");
            return {};
        }

        auto start = std::chrono::high_resolution_clock::now();
        
        // Acquire buffer from memory pool
        auto* output_buffer = memory_pool_->acquireBuffer(image_data.size());
        std::vector<uint8_t> result;
        
        if (output_buffer) {
            output_buffer->resize(image_data.size());
            std::memcpy(output_buffer->data(), image_data.data(), image_data.size());
            metrics_.recordMemoryAllocation();
        } else {
            // Fallback to direct allocation
            result = image_data;
            output_buffer = &result;
        }
        
        // Determine optimal tile size for parallel processing
        const int num_threads = std::thread::hardware_concurrency();
        const int tile_height = height / num_threads;
        
        // Process image in parallel tiles
        std::vector<std::future<void>> futures;
        
        for (int i = 0; i < num_threads; ++i) {
            int start_y = i * tile_height;
            int end_y = (i == num_threads - 1) ? height : (i + 1) * tile_height;
            
            futures.push_back(thread_pool_->enqueue([&, start_y, end_y]() {
                processor(output_buffer->data(), width, height, start_y, end_y);
            }));
        }
        
        // Wait for all tiles to complete
        for (auto& future : futures) {
            future.wait();
        }
        
        // Prepare result
        std::vector<uint8_t> final_result;
        if (output_buffer == &result) {
            final_result = std::move(result);
        } else {
            final_result = *output_buffer;
            memory_pool_->releaseBuffer(output_buffer);
        }
        
        auto end = std::chrono::high_resolution_clock::now();
        auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end - start);
        metrics_.recordOperation(duration.count());
        
        LOGI("PerformanceOptimizationEngine: Parallel processing completed (%lldms)", (long long)duration.count());
        return final_result;
    }

    // GPU memory optimization for OpenCV operations
    bool OptimizeGPUMemory() {
        if (!initialized_) return false;

#ifdef ENABLE_OPENCV
        // Clear GPU memory cache
        cv::cuda::DeviceInfo device_info;
        if (device_info.isCompatible()) {
            cv::cuda::resetDevice();
            metrics_.recordGPUOperation();
            LOGI("PerformanceOptimizationEngine: GPU memory optimized");
            return true;
        }
#endif
        return false;
    }

    // Get performance metrics
    std::string GetPerformanceReport() const {
        if (!initialized_) return "Performance engine not initialized";
        
        std::ostringstream report;
        report << "Performance Report:\\n";
        report << "Total Operations: " << metrics_.total_operations.load() << "\\n";
        report << "Average Processing Time: " << std::fixed << std::setprecision(2) 
               << metrics_.getAverageProcessingTime() << "ms\\n";
        report << "Memory Pool Size: " << memory_pool_->getPoolSize() << "\\n";
        report << "GPU Operations: " << metrics_.gpu_operations.load() << "\\n";
        report << "Memory Allocations: " << metrics_.memory_allocations.load();
        
        return report.str();
    }

    // Optimize processing pipeline for specific image sizes
    void OptimizePipelineForSize(int width, int height) {
        if (!initialized_) return;
        
        // Adjust thread pool size based on image size
        const size_t image_pixels = width * height;
        const size_t threshold_hd = 1920 * 1080;
        const size_t threshold_4k = 3840 * 2160;
        
        if (image_pixels > threshold_4k) {
            LOGI("PerformanceOptimizationEngine: Optimizing for 4K+ images");
            // Increase GPU usage for large images
            OptimizeGPUMemory();
        } else if (image_pixels > threshold_hd) {
            LOGI("PerformanceOptimizationEngine: Optimizing for HD images");
        } else {
            LOGI("PerformanceOptimizationEngine: Optimizing for standard images");
        }
    }

    void Cleanup() {
        if (initialized_) {
            LOGI("PerformanceOptimizationEngine: Cleaning up optimization systems");
            
            should_stop_.store(true);
            
            if (memory_pool_) {
                memory_pool_->cleanup();
                memory_pool_.reset();
            }
            
            if (thread_pool_) {
                thread_pool_.reset();
            }
            
            LOGI("PerformanceOptimizationEngine: Performance Report:\\n%s", 
                 GetPerformanceReport().c_str());
            
            initialized_ = false;
        }
    }
};

// Global instances
static std::unique_ptr<MediaPipeSegmenter> g_segmenter = nullptr;
static std::unique_ptr<OpenCVBlurEngine> g_blur_engine = nullptr;
static std::unique_ptr<AdvancedMaskProcessor> g_mask_processor = nullptr;
static std::unique_ptr<SmartCompositingEngine> g_compositing_engine = nullptr;
static std::unique_ptr<PerformanceOptimizationEngine> g_performance_engine = nullptr;

} // namespace blurcore

extern "C" {

// Enhanced version info with MediaPipe, OpenCV, and Advanced Mask Processing status
JNIEXPORT jstring JNICALL
Java_com_example_blurapp_BlurCore_nativeGetVersion(JNIEnv *env, jobject) {
    LOGI("BlurCore: getVersion called");
    
    std::string version = "BlurCore v5.0.0";
    
#ifdef ENABLE_MEDIAPIPE
    version += " (MediaPipe enabled)";
#else
    version += " (MediaPipe disabled)";
#endif

#ifdef ENABLE_OPENCV
    version += " (OpenCV enabled";
#ifdef ENABLE_OPENCV_GPU
    version += " + GPU)";
#else
    version += " - CPU only)";
#endif
    version += " (Advanced Mask Processing)";
#else
    version += " (OpenCV disabled - fallback mode)";
#endif

    return env->NewStringUTF(version.c_str());
}

// Phase 1: Check MediaPipe availability
JNIEXPORT jboolean JNICALL
Java_com_example_blurapp_BlurCore_nativeIsMediaPipeAvailable(JNIEnv *env, jobject) {
    LOGI("BlurCore: Checking MediaPipe availability");
    
#ifdef ENABLE_MEDIAPIPE
    return JNI_TRUE;
#else
    return JNI_FALSE;
#endif
}

// Phase 1: Initialize MediaPipe segmentation
JNIEXPORT jboolean JNICALL
Java_com_example_blurapp_BlurCore_nativeInitializeSegmentation(JNIEnv *env, jobject, jstring model_path) {
    LOGI("BlurCore: Initializing segmentation");
    
    if (blurcore::g_segmenter == nullptr) {
        blurcore::g_segmenter = std::make_unique<blurcore::MediaPipeSegmenter>();
    }
    
    const char* path_chars = env->GetStringUTFChars(model_path, nullptr);
    std::string path(path_chars);
    env->ReleaseStringUTFChars(model_path, path_chars);
    
    bool success = blurcore::g_segmenter->Initialize(path);
    LOGI("BlurCore: Segmentation initialization %s", success ? "succeeded" : "failed");
    
    return success ? JNI_TRUE : JNI_FALSE;
}

// Phase 1: Perform image segmentation
JNIEXPORT jbyteArray JNICALL
Java_com_example_blurapp_BlurCore_nativeSegmentImage(JNIEnv *env, jobject, jbyteArray image_bytes, jint width, jint height) {
    LOGI("BlurCore: segmentImage called for %dx%d image", width, height);
    
    if (blurcore::g_segmenter == nullptr || !blurcore::g_segmenter->IsInitialized()) {
        LOGI("BlurCore: Segmenter not initialized, returning empty result");
        return env->NewByteArray(0);
    }
    
    // Convert Java byte array to C++ vector
    jsize input_length = env->GetArrayLength(image_bytes);
    jbyte* input_data = env->GetByteArrayElements(image_bytes, nullptr);
    
    std::vector<uint8_t> image_data(input_data, input_data + input_length);
    env->ReleaseByteArrayElements(image_bytes, input_data, JNI_ABORT);
    
    // Perform segmentation
    auto mask_data = blurcore::g_segmenter->Segment(image_data, width, height);
    
    // Convert result back to Java byte array
    jbyteArray result = env->NewByteArray(mask_data.size());
    if (!mask_data.empty()) {
        env->SetByteArrayRegion(result, 0, mask_data.size(), 
                               reinterpret_cast<const jbyte*>(mask_data.data()));
    }
    
    LOGI("BlurCore: Segmentation returned %zu bytes", mask_data.size());
    return result;
}

// Phase 2: Enhanced image processing with OpenCV blur engine
JNIEXPORT jbyteArray JNICALL
Java_com_example_blurapp_BlurCore_nativeProcessImageBasic(JNIEnv *env, jobject, jbyteArray input_bytes, jint blur_strength) {
    LOGI("BlurCore: Enhanced processing with strength %d", blur_strength);
    
    // Initialize blur engine if needed
    if (blurcore::g_blur_engine == nullptr) {
        blurcore::g_blur_engine = std::make_unique<blurcore::OpenCVBlurEngine>();
        blurcore::g_blur_engine->Initialize();
    }
    
    jsize input_length = env->GetArrayLength(input_bytes);
    jbyte* input_data = env->GetByteArrayElements(input_bytes, nullptr);
    
    // Convert to std::vector for processing
    std::vector<uint8_t> image_data(input_data, input_data + input_length);
    env->ReleaseByteArrayElements(input_bytes, input_data, JNI_ABORT);
    
    std::vector<uint8_t> processed_data;
    
    if (blurcore::g_blur_engine && blurcore::g_blur_engine->IsInitialized() && blur_strength > 0) {
        // Phase 2: Use OpenCV for high-quality blur
        
        // Calculate sigma from blur strength (1-100 -> 0.5-15.0)
        double sigma = 0.5 + (blur_strength / 100.0) * 14.5;
        
        // Assume RGBA format for now (4 channels) - this should be parameterized in production
        int estimated_width = static_cast<int>(std::sqrt(input_length / 4));
        int estimated_height = estimated_width;
        
        // For Phase 2, we'll use estimated dimensions
        // In Phase 3, we'll add proper image format detection
        processed_data = blurcore::g_blur_engine->ApplyGaussianBlur(
            image_data, estimated_width, estimated_height, 4, sigma, 0);
            
        LOGI("BlurCore: OpenCV blur applied (sigma: %.2f, GPU: %s)", 
             sigma, blurcore::g_blur_engine->IsGPUAvailable() ? "yes" : "no");
    } else {
        // Fallback: return original data
        processed_data = image_data;
        LOGI("BlurCore: Using fallback mode");
    }
    
    // Convert result back to Java byte array
    jbyteArray result = env->NewByteArray(processed_data.size());
    env->SetByteArrayRegion(result, 0, processed_data.size(), 
                           reinterpret_cast<const jbyte*>(processed_data.data()));
    
    LOGI("BlurCore: Enhanced processing completed (%zu bytes)", processed_data.size());
    return result;
}

// Phase 2: Advanced blur with parameters
JNIEXPORT jbyteArray JNICALL
Java_com_example_blurapp_BlurCore_nativeApplyAdvancedBlur(JNIEnv *env, jobject, 
                                                         jbyteArray input_bytes, 
                                                         jint width, jint height, jint channels,
                                                         jdouble sigma, jint blur_type) {
    LOGI("BlurCore: Advanced blur (%dx%d, sigma=%.2f, type=%d)", width, height, sigma, blur_type);
    
    // Initialize blur engine if needed
    if (blurcore::g_blur_engine == nullptr) {
        blurcore::g_blur_engine = std::make_unique<blurcore::OpenCVBlurEngine>();
        blurcore::g_blur_engine->Initialize();
    }
    
    if (!blurcore::g_blur_engine || !blurcore::g_blur_engine->IsInitialized()) {
        LOGE("BlurCore: Blur engine not available");
        return env->NewByteArray(0);
    }
    
    jsize input_length = env->GetArrayLength(input_bytes);
    jbyte* input_data = env->GetByteArrayElements(input_bytes, nullptr);
    
    std::vector<uint8_t> image_data(input_data, input_data + input_length);
    env->ReleaseByteArrayElements(input_bytes, input_data, JNI_ABORT);
    
    // Apply advanced blur
    auto result_data = blurcore::g_blur_engine->ApplyGaussianBlur(
        image_data, width, height, channels, sigma, blur_type);
    
    // Convert result back to Java byte array
    jbyteArray result = env->NewByteArray(result_data.size());
    env->SetByteArrayRegion(result, 0, result_data.size(), 
                           reinterpret_cast<const jbyte*>(result_data.data()));
    
    return result;
}

// Phase 2: Selective blur using segmentation mask
JNIEXPORT jbyteArray JNICALL
Java_com_example_blurapp_BlurCore_nativeApplySelectiveBlur(JNIEnv *env, jobject,
                                                          jbyteArray input_bytes,
                                                          jbyteArray mask_bytes,
                                                          jint width, jint height, jint channels,
                                                          jdouble fg_sigma, jdouble bg_sigma) {
    LOGI("BlurCore: Selective blur (%dx%d, fg=%.2f, bg=%.2f)", width, height, fg_sigma, bg_sigma);
    
    // Initialize blur engine if needed
    if (blurcore::g_blur_engine == nullptr) {
        blurcore::g_blur_engine = std::make_unique<blurcore::OpenCVBlurEngine>();
        blurcore::g_blur_engine->Initialize();
    }
    
    if (!blurcore::g_blur_engine || !blurcore::g_blur_engine->IsInitialized()) {
        LOGE("BlurCore: Blur engine not available");
        return env->NewByteArray(0);
    }
    
    // Get image data
    jsize input_length = env->GetArrayLength(input_bytes);
    jbyte* input_data = env->GetByteArrayElements(input_bytes, nullptr);
    std::vector<uint8_t> image_data(input_data, input_data + input_length);
    env->ReleaseByteArrayElements(input_bytes, input_data, JNI_ABORT);
    
    // Get mask data
    jsize mask_length = env->GetArrayLength(mask_bytes);
    jbyte* mask_data = env->GetByteArrayElements(mask_bytes, nullptr);
    std::vector<uint8_t> mask_vector(mask_data, mask_data + mask_length);
    env->ReleaseByteArrayElements(mask_bytes, mask_data, JNI_ABORT);
    
    // Apply selective blur
    auto result_data = blurcore::g_blur_engine->ApplySelectiveBlur(
        image_data, mask_vector, width, height, channels, fg_sigma, bg_sigma);
    
    // Convert result back to Java byte array
    jbyteArray result = env->NewByteArray(result_data.size());
    env->SetByteArrayRegion(result, 0, result_data.size(), 
                           reinterpret_cast<const jbyte*>(result_data.data()));
    
    return result;
}

// Phase 2: Check OpenCV availability
JNIEXPORT jboolean JNICALL
Java_com_example_blurapp_BlurCore_nativeIsOpenCVAvailable(JNIEnv *env, jobject) {
    LOGI("BlurCore: Checking OpenCV availability");
    
#ifdef ENABLE_OPENCV
    return JNI_TRUE;
#else
    return JNI_FALSE;
#endif
}

// Phase 2: Check GPU acceleration availability
JNIEXPORT jboolean JNICALL
Java_com_example_blurapp_BlurCore_nativeIsGPUAvailable(JNIEnv *env, jobject) {
    LOGI("BlurCore: Checking GPU availability");
    
    if (blurcore::g_blur_engine == nullptr) {
        blurcore::g_blur_engine = std::make_unique<blurcore::OpenCVBlurEngine>();
        blurcore::g_blur_engine->Initialize();
    }
    
    return (blurcore::g_blur_engine && blurcore::g_blur_engine->IsGPUAvailable()) ? JNI_TRUE : JNI_FALSE;
}

// Phase 2: Enhanced cleanup with blur engine
JNIEXPORT void JNICALL
Java_com_example_blurapp_BlurCore_nativeCleanup(JNIEnv *env, jobject) {
    LOGI("BlurCore: Enhanced cleanup called");
    
    if (blurcore::g_segmenter != nullptr) {
        blurcore::g_segmenter->Cleanup();
        blurcore::g_segmenter.reset();
    }
    
    if (blurcore::g_blur_engine != nullptr) {
        blurcore::g_blur_engine->Cleanup();
        blurcore::g_blur_engine.reset();
    }
    
    if (blurcore::g_mask_processor != nullptr) {
        blurcore::g_mask_processor.reset();
    }
    
    if (blurcore::g_compositing_engine != nullptr) {
        blurcore::g_compositing_engine->Cleanup();
        blurcore::g_compositing_engine.reset();
    }
    
    if (blurcore::g_performance_engine != nullptr) {
        blurcore::g_performance_engine->Cleanup();
        blurcore::g_performance_engine.reset();
    }
    
    LOGI("BlurCore: Enhanced cleanup completed");
}

// ================================================================================
// Phase 3: Advanced Mask Processing JNI Functions
// ================================================================================

// Phase 3: Refine mask using morphological operations
JNIEXPORT jbyteArray JNICALL
Java_com_example_blurapp_BlurCore_nativeRefineMask(JNIEnv *env, jobject, 
                                                   jbyteArray mask_bytes, 
                                                   jint width, jint height,
                                                   jstring operation_type, 
                                                   jint kernel_size) {
    LOGI("BlurCore: Refining mask (%dx%d, kernel=%d)", width, height, kernel_size);
    
    // Initialize mask processor if needed
    if (blurcore::g_mask_processor == nullptr) {
        blurcore::g_mask_processor = std::make_unique<blurcore::AdvancedMaskProcessor>();
    }
    
    if (!blurcore::g_mask_processor) {
        LOGE("BlurCore: Mask processor not available");
        return env->NewByteArray(0);
    }
    
    // Convert operation type from Java string
    const char* op_str = env->GetStringUTFChars(operation_type, nullptr);
    std::string operation(op_str);
    env->ReleaseStringUTFChars(operation_type, op_str);
    
    // Extract mask data
    jsize mask_length = env->GetArrayLength(mask_bytes);
    jbyte* mask_ptr = env->GetByteArrayElements(mask_bytes, nullptr);
    
    std::vector<uint8_t> mask_data(mask_ptr, mask_ptr + mask_length);
    env->ReleaseByteArrayElements(mask_bytes, mask_ptr, JNI_ABORT);
    
    // Process mask
    std::vector<uint8_t> refined_mask = blurcore::g_mask_processor->RefineMask(
        mask_data, width, height, operation, kernel_size);
    
    if (refined_mask.empty()) {
        LOGE("BlurCore: Mask refinement failed");
        return env->NewByteArray(0);
    }
    
    // Convert result back to Java byte array
    jbyteArray result = env->NewByteArray(refined_mask.size());
    env->SetByteArrayRegion(result, 0, refined_mask.size(), 
                           reinterpret_cast<const jbyte*>(refined_mask.data()));
    
    LOGI("BlurCore: Mask refinement completed (%zu bytes)", refined_mask.size());
    return result;
}

// Phase 3: Smooth mask edges
JNIEXPORT jbyteArray JNICALL
Java_com_example_blurapp_BlurCore_nativeSmoothMaskEdges(JNIEnv *env, jobject, 
                                                       jbyteArray mask_bytes, 
                                                       jint width, jint height,
                                                       jdouble blur_sigma) {
    LOGI("BlurCore: Smoothing mask edges (%dx%d, sigma=%.2f)", width, height, blur_sigma);
    
    // Initialize mask processor if needed
    if (blurcore::g_mask_processor == nullptr) {
        blurcore::g_mask_processor = std::make_unique<blurcore::AdvancedMaskProcessor>();
    }
    
    if (!blurcore::g_mask_processor) {
        LOGE("BlurCore: Mask processor not available");
        return env->NewByteArray(0);
    }
    
    // Extract mask data
    jsize mask_length = env->GetArrayLength(mask_bytes);
    jbyte* mask_ptr = env->GetByteArrayElements(mask_bytes, nullptr);
    
    std::vector<uint8_t> mask_data(mask_ptr, mask_ptr + mask_length);
    env->ReleaseByteArrayElements(mask_bytes, mask_ptr, JNI_ABORT);
    
    // Process mask
    std::vector<uint8_t> smoothed_mask = blurcore::g_mask_processor->SmoothMaskEdges(
        mask_data, width, height, blur_sigma);
    
    if (smoothed_mask.empty()) {
        LOGE("BlurCore: Mask edge smoothing failed");
        return env->NewByteArray(0);
    }
    
    // Convert result back to Java byte array
    jbyteArray result = env->NewByteArray(smoothed_mask.size());
    env->SetByteArrayRegion(result, 0, smoothed_mask.size(), 
                           reinterpret_cast<const jbyte*>(smoothed_mask.data()));
    
    LOGI("BlurCore: Mask edge smoothing completed (%zu bytes)", smoothed_mask.size());
    return result;
}

// Phase 3: Optimize mask using connected components
JNIEXPORT jbyteArray JNICALL
Java_com_example_blurapp_BlurCore_nativeOptimizeMask(JNIEnv *env, jobject, 
                                                     jbyteArray mask_bytes, 
                                                     jint width, jint height,
                                                     jint min_area) {
    LOGI("BlurCore: Optimizing mask (%dx%d, min_area=%d)", width, height, min_area);
    
    // Initialize mask processor if needed
    if (blurcore::g_mask_processor == nullptr) {
        blurcore::g_mask_processor = std::make_unique<blurcore::AdvancedMaskProcessor>();
    }
    
    if (!blurcore::g_mask_processor) {
        LOGE("BlurCore: Mask processor not available");
        return env->NewByteArray(0);
    }
    
    // Extract mask data
    jsize mask_length = env->GetArrayLength(mask_bytes);
    jbyte* mask_ptr = env->GetByteArrayElements(mask_bytes, nullptr);
    
    std::vector<uint8_t> mask_data(mask_ptr, mask_ptr + mask_length);
    env->ReleaseByteArrayElements(mask_bytes, mask_ptr, JNI_ABORT);
    
    // Process mask
    std::vector<uint8_t> optimized_mask = blurcore::g_mask_processor->OptimizeMask(
        mask_data, width, height, min_area);
    
    if (optimized_mask.empty()) {
        LOGE("BlurCore: Mask optimization failed");
        return env->NewByteArray(0);
    }
    
    // Convert result back to Java byte array
    jbyteArray result = env->NewByteArray(optimized_mask.size());
    env->SetByteArrayRegion(result, 0, optimized_mask.size(), 
                           reinterpret_cast<const jbyte*>(optimized_mask.data()));
    
    LOGI("BlurCore: Mask optimization completed (%zu bytes)", optimized_mask.size());
    return result;
}

// Phase 3: Create feathered mask
JNIEXPORT jbyteArray JNICALL
Java_com_example_blurapp_BlurCore_nativeCreateFeatheredMask(JNIEnv *env, jobject, 
                                                           jbyteArray mask_bytes, 
                                                           jint width, jint height,
                                                           jint feather_radius) {
    LOGI("BlurCore: Creating feathered mask (%dx%d, radius=%d)", width, height, feather_radius);
    
    // Initialize mask processor if needed
    if (blurcore::g_mask_processor == nullptr) {
        blurcore::g_mask_processor = std::make_unique<blurcore::AdvancedMaskProcessor>();
    }
    
    if (!blurcore::g_mask_processor) {
        LOGE("BlurCore: Mask processor not available");
        return env->NewByteArray(0);
    }
    
    // Extract mask data
    jsize mask_length = env->GetArrayLength(mask_bytes);
    jbyte* mask_ptr = env->GetByteArrayElements(mask_bytes, nullptr);
    
    std::vector<uint8_t> mask_data(mask_ptr, mask_ptr + mask_length);
    env->ReleaseByteArrayElements(mask_bytes, mask_ptr, JNI_ABORT);
    
    // Process mask
    std::vector<uint8_t> feathered_mask = blurcore::g_mask_processor->CreateFeatheredMask(
        mask_data, width, height, feather_radius);
    
    if (feathered_mask.empty()) {
        LOGE("BlurCore: Mask feathering failed");
        return env->NewByteArray(0);
    }
    
    // Convert result back to Java byte array
    jbyteArray result = env->NewByteArray(feathered_mask.size());
    env->SetByteArrayRegion(result, 0, feathered_mask.size(), 
                           reinterpret_cast<const jbyte*>(feathered_mask.data()));
    
    LOGI("BlurCore: Mask feathering completed (%zu bytes)", feathered_mask.size());
    return result;
}

// ================================================================================
// Phase 4: Smart Compositing Engine JNI Functions
// ================================================================================

// Phase 4: Multi-layer alpha blending with smart edge preservation
JNIEXPORT jbyteArray JNICALL
Java_com_example_blurapp_BlurCore_nativeBlendLayers(JNIEnv *env, jobject, 
                                                    jbyteArray base_bytes, 
                                                    jbyteArray overlay_bytes,
                                                    jbyteArray mask_bytes,
                                                    jint width, jint height,
                                                    jdouble blend_strength) {
    LOGI("BlurCore: Blending layers (%dx%d, strength=%.2f)", width, height, blend_strength);
    
    // Initialize compositing engine if needed
    if (blurcore::g_compositing_engine == nullptr) {
        blurcore::g_compositing_engine = std::make_unique<blurcore::SmartCompositingEngine>();
        blurcore::g_compositing_engine->Initialize();
    }
    
    if (!blurcore::g_compositing_engine || !blurcore::g_compositing_engine->IsInitialized()) {
        LOGE("BlurCore: Compositing engine not available");
        return env->NewByteArray(0);
    }
    
    // Extract image data
    jsize base_length = env->GetArrayLength(base_bytes);
    jbyte* base_ptr = env->GetByteArrayElements(base_bytes, nullptr);
    std::vector<uint8_t> base_data(base_ptr, base_ptr + base_length);
    env->ReleaseByteArrayElements(base_bytes, base_ptr, JNI_ABORT);
    
    jsize overlay_length = env->GetArrayLength(overlay_bytes);
    jbyte* overlay_ptr = env->GetByteArrayElements(overlay_bytes, nullptr);
    std::vector<uint8_t> overlay_data(overlay_ptr, overlay_ptr + overlay_length);
    env->ReleaseByteArrayElements(overlay_bytes, overlay_ptr, JNI_ABORT);
    
    jsize mask_length = env->GetArrayLength(mask_bytes);
    jbyte* mask_ptr = env->GetByteArrayElements(mask_bytes, nullptr);
    std::vector<uint8_t> mask_data(mask_ptr, mask_ptr + mask_length);
    env->ReleaseByteArrayElements(mask_bytes, mask_ptr, JNI_ABORT);
    
    // Blend layers
    std::vector<uint8_t> blended_result = blurcore::g_compositing_engine->BlendLayers(
        base_data, overlay_data, mask_data, width, height, blend_strength);
    
    if (blended_result.empty()) {
        LOGE("BlurCore: Layer blending failed");
        return env->NewByteArray(0);
    }
    
    // Convert result back to Java byte array
    jbyteArray result = env->NewByteArray(blended_result.size());
    env->SetByteArrayRegion(result, 0, blended_result.size(), 
                           reinterpret_cast<const jbyte*>(blended_result.data()));
    
    LOGI("BlurCore: Layer blending completed (%zu bytes)", blended_result.size());
    return result;
}

// Phase 4: Advanced color space blending
JNIEXPORT jbyteArray JNICALL
Java_com_example_blurapp_BlurCore_nativeAdvancedColorBlend(JNIEnv *env, jobject, 
                                                          jbyteArray base_bytes, 
                                                          jbyteArray overlay_bytes,
                                                          jbyteArray mask_bytes,
                                                          jint width, jint height,
                                                          jstring color_space) {
    LOGI("BlurCore: Advanced color blending (%dx%d)", width, height);
    
    // Initialize compositing engine if needed
    if (blurcore::g_compositing_engine == nullptr) {
        blurcore::g_compositing_engine = std::make_unique<blurcore::SmartCompositingEngine>();
        blurcore::g_compositing_engine->Initialize();
    }
    
    if (!blurcore::g_compositing_engine || !blurcore::g_compositing_engine->IsInitialized()) {
        LOGE("BlurCore: Compositing engine not available");
        return env->NewByteArray(0);
    }
    
    // Convert color space from Java string
    const char* cs_str = env->GetStringUTFChars(color_space, nullptr);
    std::string cs_string(cs_str);
    env->ReleaseStringUTFChars(color_space, cs_str);
    
    // Extract image data
    jsize base_length = env->GetArrayLength(base_bytes);
    jbyte* base_ptr = env->GetByteArrayElements(base_bytes, nullptr);
    std::vector<uint8_t> base_data(base_ptr, base_ptr + base_length);
    env->ReleaseByteArrayElements(base_bytes, base_ptr, JNI_ABORT);
    
    jsize overlay_length = env->GetArrayLength(overlay_bytes);
    jbyte* overlay_ptr = env->GetByteArrayElements(overlay_bytes, nullptr);
    std::vector<uint8_t> overlay_data(overlay_ptr, overlay_ptr + overlay_length);
    env->ReleaseByteArrayElements(overlay_bytes, overlay_ptr, JNI_ABORT);
    
    jsize mask_length = env->GetArrayLength(mask_bytes);
    jbyte* mask_ptr = env->GetByteArrayElements(mask_bytes, nullptr);
    std::vector<uint8_t> mask_data(mask_ptr, mask_ptr + mask_length);
    env->ReleaseByteArrayElements(mask_bytes, mask_ptr, JNI_ABORT);
    
    // Apply advanced color blending
    std::vector<uint8_t> blended_result = blurcore::g_compositing_engine->AdvancedColorBlend(
        base_data, overlay_data, mask_data, width, height, cs_string);
    
    if (blended_result.empty()) {
        LOGE("BlurCore: Advanced color blending failed");
        return env->NewByteArray(0);
    }
    
    // Convert result back to Java byte array
    jbyteArray result = env->NewByteArray(blended_result.size());
    env->SetByteArrayRegion(result, 0, blended_result.size(), 
                           reinterpret_cast<const jbyte*>(blended_result.data()));
    
    LOGI("BlurCore: Advanced color blending completed (%zu bytes)", blended_result.size());
    return result;
}

// Phase 4: Gradient domain compositing
JNIEXPORT jbyteArray JNICALL
Java_com_example_blurapp_BlurCore_nativeGradientDomainComposite(JNIEnv *env, jobject, 
                                                               jbyteArray base_bytes, 
                                                               jbyteArray overlay_bytes,
                                                               jbyteArray mask_bytes,
                                                               jint width, jint height) {
    LOGI("BlurCore: Gradient domain compositing (%dx%d)", width, height);
    
    // Initialize compositing engine if needed
    if (blurcore::g_compositing_engine == nullptr) {
        blurcore::g_compositing_engine = std::make_unique<blurcore::SmartCompositingEngine>();
        blurcore::g_compositing_engine->Initialize();
    }
    
    if (!blurcore::g_compositing_engine || !blurcore::g_compositing_engine->IsInitialized()) {
        LOGE("BlurCore: Compositing engine not available");
        return env->NewByteArray(0);
    }
    
    // Extract image data
    jsize base_length = env->GetArrayLength(base_bytes);
    jbyte* base_ptr = env->GetByteArrayElements(base_bytes, nullptr);
    std::vector<uint8_t> base_data(base_ptr, base_ptr + base_length);
    env->ReleaseByteArrayElements(base_bytes, base_ptr, JNI_ABORT);
    
    jsize overlay_length = env->GetArrayLength(overlay_bytes);
    jbyte* overlay_ptr = env->GetByteArrayElements(overlay_bytes, nullptr);
    std::vector<uint8_t> overlay_data(overlay_ptr, overlay_ptr + overlay_length);
    env->ReleaseByteArrayElements(overlay_bytes, overlay_ptr, JNI_ABORT);
    
    jsize mask_length = env->GetArrayLength(mask_bytes);
    jbyte* mask_ptr = env->GetByteArrayElements(mask_bytes, nullptr);
    std::vector<uint8_t> mask_data(mask_ptr, mask_ptr + mask_length);
    env->ReleaseByteArrayElements(mask_bytes, mask_ptr, JNI_ABORT);
    
    // Apply gradient domain compositing
    std::vector<uint8_t> composite_result = blurcore::g_compositing_engine->GradientDomainComposite(
        base_data, overlay_data, mask_data, width, height);
    
    if (composite_result.empty()) {
        LOGE("BlurCore: Gradient domain compositing failed");
        return env->NewByteArray(0);
    }
    
    // Convert result back to Java byte array
    jbyteArray result = env->NewByteArray(composite_result.size());
    env->SetByteArrayRegion(result, 0, composite_result.size(), 
                           reinterpret_cast<const jbyte*>(composite_result.data()));
    
    LOGI("BlurCore: Gradient domain compositing completed (%zu bytes)", composite_result.size());
    return result;
}

// ================================================================================
// Phase 5: Performance Optimization JNI Functions
// ================================================================================

// Phase 5: Get performance metrics and optimize system
JNIEXPORT jstring JNICALL
Java_com_example_blurapp_BlurCore_nativeGetPerformanceReport(JNIEnv *env, jobject) {
    LOGI("BlurCore: Getting performance report");
    
    // Initialize performance engine if needed
    if (blurcore::g_performance_engine == nullptr) {
        blurcore::g_performance_engine = std::make_unique<blurcore::PerformanceOptimizationEngine>();
        blurcore::g_performance_engine->Initialize();
    }
    
    if (!blurcore::g_performance_engine || !blurcore::g_performance_engine->IsInitialized()) {
        return env->NewStringUTF("Performance engine not available");
    }
    
    std::string report = blurcore::g_performance_engine->GetPerformanceReport();
    return env->NewStringUTF(report.c_str());
}

// Phase 5: Optimize processing pipeline for specific image dimensions
JNIEXPORT jboolean JNICALL
Java_com_example_blurapp_BlurCore_nativeOptimizePipeline(JNIEnv *env, jobject, 
                                                        jint width, jint height) {
    LOGI("BlurCore: Optimizing pipeline for %dx%d", width, height);
    
    // Initialize performance engine if needed
    if (blurcore::g_performance_engine == nullptr) {
        blurcore::g_performance_engine = std::make_unique<blurcore::PerformanceOptimizationEngine>();
        blurcore::g_performance_engine->Initialize();
    }
    
    if (!blurcore::g_performance_engine || !blurcore::g_performance_engine->IsInitialized()) {
        LOGE("BlurCore: Performance engine not available");
        return JNI_FALSE;
    }
    
    blurcore::g_performance_engine->OptimizePipelineForSize(width, height);
    blurcore::g_performance_engine->OptimizeGPUMemory();
    
    LOGI("BlurCore: Pipeline optimization completed");
    return JNI_TRUE;
}

} // extern "C"